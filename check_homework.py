import streamlit as st
import pandas as pd
import re
import io
import os
import datetime
import hashlib

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ä½œä¸šå¤šç»´åº¦åˆ†æç³»ç»Ÿ", layout="wide")
st.title("ğŸ“ ä½œä¸šæäº¤å¤šç»´åº¦åˆ†æç³»ç»Ÿ")

# --- æ ¸å¿ƒå¤„ç†å‡½æ•° ---
def extract_id(filename):
    """ä»æ–‡ä»¶åæå–9ä½å­¦å·"""
    match = re.search(r'\d{9}', filename)
    return match.group() if match else None

def calculate_bytes_md5(file_bytes):
    """è®¡ç®—ä¸Šä¼ æ–‡ä»¶æµçš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    hash_md5.update(file_bytes)
    return hash_md5.hexdigest()

def get_roster_from_upload(uploaded_file):
    """ä»ä¸Šä¼ çš„Excelä¸­è‡ªåŠ¨è¯†åˆ«å­¦å·å’Œå§“ååˆ—"""
    try:
        df = pd.read_excel(uploaded_file)
        # å¯»æ‰¾å­¦å·åˆ—ç´¢å¼•
        sid_idx = next((i for i, col in enumerate(df.columns) if 'å­¦å·' in str(col)), None)
        if sid_idx is None:
            for i, col in enumerate(df.columns):
                if any(re.search(r'\d{9}', str(v)) for v in df[col].dropna().head(5)):
                    sid_idx = i
                    break
        if sid_idx is None: return None, "Excelä¸­æœªæ‰¾åˆ°å­¦å·åˆ—"
        
        # å§“ååˆ—ä¸ºå­¦å·åä¸€åˆ—
        name_idx = sid_idx + 1
        roster = {}
        for _, row in df.iterrows():
            sid_match = re.search(r'\d{9}', str(row[df.columns[sid_idx]]))
            if sid_match:
                s_id = sid_match.group()
                s_name = str(row[df.columns[name_idx]]) if name_idx < len(df.columns) else "æœªçŸ¥"
                roster[s_id] = s_name
        return roster, None
    except Exception as e:
        return None, str(e)

# --- ä¾§è¾¹æ ï¼šæ‰‹åŠ¨ä¸Šä¼ åŒº ---
with st.sidebar:
    st.header("ğŸ“ æ•°æ®ä¸Šä¼ ")
    uploaded_roster = st.file_uploader("1. ä¸Šä¼ èŠ±åå†Œ (Excel)", type=['xlsx'])
    
    # å…è®¸ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ï¼Œç”šè‡³å¯ä»¥ç›´æ¥æŠŠæ•´ä¸ªæ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶æ‹–è¿›æ¥
    uploaded_homeworks = st.file_uploader("2. ä¸Šä¼ ä½œä¸šæ–‡ä»¶ (å¯å¤šé€‰/å…¨é€‰æ‹–å…¥)", 
                                         type=['py', 'zip', 'txt', 'docx', 'pdf'], 
                                         accept_multiple_files=True)
    
    st.divider()
    st.header("âš™ï¸ ä»»åŠ¡è®¾ç½®")
    deadline_date = st.date_input("æˆªæ­¢æ—¥æœŸ", datetime.date.today())
    deadline_time = st.time_input("æˆªæ­¢æ—¶é—´", datetime.time(23, 59))
    deadline = datetime.datetime.combine(deadline_date, deadline_time)

# --- ä¸»ç•Œé¢é€»è¾‘ ---
if uploaded_roster and uploaded_homeworks:
    roster_dict, err = get_roster_from_upload(uploaded_roster)
    
    if err:
        st.error(f"èŠ±åå†Œè¯»å–å¤±è´¥: {err}")
    else:
        all_roster_ids = set(roster_dict.keys())
        
        # åˆå§‹åŒ–åˆ†æå®¹å™¨
        analysis = {
            "valid": {},    # åˆè§„æäº¤ {å­¦å·: [æ–‡ä»¶ä¿¡æ¯]}
            "unknown": [],  # æ— æ³•åŒ¹é…çš„æ–‡ä»¶
            "similarity": {} # MD5: [æ–‡ä»¶ä¿¡æ¯]
        }

        # å¤„ç†æ¯ä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶
        for uploaded_file in uploaded_homeworks:
            sid = extract_id(uploaded_file.name)
            # è·å–ä¸Šä¼ æ–‡ä»¶æµçš„MD5
            file_bytes = uploaded_file.getvalue()
            md5_hash = calculate_bytes_md5(file_bytes)
            
            # Streamlitä¸Šä¼ æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´é€šå¸¸æ˜¯ä¸Šä¼ æ—¶é—´ï¼Œ
            # è¿™é‡Œçš„â€œè¿Ÿäº¤åˆ†æâ€æ›´é€‚åˆæœ¬åœ°ç¯å¢ƒï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡ç•Œé¢è®¾ç½®æ¥æ¨¡æ‹Ÿã€‚
            # æ³¨æ„ï¼šWebç¯å¢ƒä¸‹æ— æ³•ç›´æ¥è·å–åŸæ–‡ä»¶çš„æœ¬åœ°ä¿®æ”¹æ—¶é—´ï¼Œé€šå¸¸ä»¥å½“å‰æ—¶é—´ä¸ºå‡†
            now = datetime.datetime.now()
            is_late = now > deadline

            file_info = {
                "name": uploaded_file.name,
                "time": now,
                "is_late": is_late,
                "md5": md5_hash,
                "size": uploaded_file.size
            }

            if not sid or sid not in all_roster_ids:
                analysis["unknown"].append(file_info)
            else:
                if sid not in analysis["valid"]:
                    analysis["valid"][sid] = []
                analysis["valid"][sid].append(file_info)
            
            # è®°å½•ç›¸ä¼¼åº¦
            if md5_hash not in analysis["similarity"]:
                analysis["similarity"][md5_hash] = []
            analysis["similarity"][md5_hash].append(file_info)

        # --- æ•°æ®å±•ç¤º ---
        st.divider()
        submitted_count = len(analysis["valid"])
        total_count = len(all_roster_ids)
        percent = int(submitted_count / total_count * 100) if total_count > 0 else 0
        
        c1, c2, c3 = st.columns([1, 1, 2])
        c1.metric("åŒ¹é…èŠ±åå†Œäººæ•°", f"{submitted_count} / {total_count}")
        c2.metric("å®Œæˆç‡", f"{percent}%")
        with c3:
            st.write("ç­çº§æäº¤è¿›åº¦")
            st.progress(percent / 100)

        # é¡µç­¾å¸ƒå±€
        t1, t2, t3, t4 = st.tabs(["âŒ æœªäº¤åå•", "âœ… å·²äº¤åˆ†æ", "âš ï¸ å¼‚å¸¸/é‡å¤", "ğŸ‘¥ ç›¸ä¼¼åº¦åˆç­›"])

        with t1:
            missing_ids = sorted(list(all_roster_ids - set(analysis["valid"].keys())))
            if missing_ids:
                df_missing = pd.DataFrame([{"å­¦å·": i, "å§“å": roster_dict[i]} for i in missing_ids])
                st.dataframe(df_missing, use_container_width=True)
                
                # ä¸‹è½½æœªäº¤åå•
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df_missing.to_excel(writer, index=False)
                st.download_button("ğŸ“¥ ä¸‹è½½æœªäº¤åå•Excel", output.getvalue(), "æœªäº¤åå•.xlsx")
            else:
                st.success("ğŸ‰ å…¨å‘˜äº¤é½ï¼")

        with t2:
            st.markdown("### å·²äº¤æƒ…å†µ")
            done_data = []
            for sid, f_list in analysis["valid"].items():
                # è‹¥æœ‰å¤šæ–‡ä»¶ï¼Œæ˜¾ç¤ºæœ€æ–°ä¸Šä¼ çš„ä¸€ä¸ª
                f = f_list[-1]
                done_data.append({
                    "å­¦å·": sid,
                    "å§“å": roster_dict[sid],
                    "æ–‡ä»¶å": f["name"],
                    "å¤§å°(KB)": round(f["size"]/1024, 2),
                    "ç‰ˆæœ¬æ•°": len(f_list)
                })
            st.dataframe(pd.DataFrame(done_data).sort_values("å­¦å·"), use_container_width=True, hide_index=True)

        with t3:
            col_a, col_b = st.columns(2)
            with col_a:
                st.subheader("â“ æ— æ³•è¯†åˆ«/ä¸åœ¨åå†Œ")
                if analysis["unknown"]:
                    for f in analysis["unknown"]:
                        st.write(f"- {f['name']}")
                else:
                    st.write("æ— å¼‚å¸¸")
            with col_b:
                st.subheader("ğŸ‘¯ é‡å¤æäº¤")
                dups = {sid: flist for sid, flist in analysis["valid"].items() if len(flist) > 1}
                for sid, flist in dups.items():
                    st.warning(f"{sid} ({roster_dict[sid]}) æäº¤äº† {len(flist)} ä¸ªæ–‡ä»¶")

        with t4:
            st.subheader("ğŸ‘¥ å†…å®¹å®Œå…¨ä¸€è‡´æ£€æµ‹ (MD5)")
            found_sim = False
            for md5, flist in analysis["similarity"].items():
                if len(flist) > 1:
                    found_sim = True
                    st.error(f"å†…å®¹æŒ‡çº¹ [{md5[:8]}] å®Œå…¨ä¸€è‡´çš„æ–‡ä»¶ï¼š")
                    for f in flist:
                        st.write(f"  - {f['name']}")
            if not found_sim:
                st.info("æœªå‘ç°å®Œå…¨ç›¸åŒçš„æ–‡ä»¶å†…å®¹ã€‚")

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ã€èŠ±åå†Œã€‘å’Œã€ä½œä¸šæ–‡ä»¶ã€‘ã€‚ä½ å¯ä»¥ä¸€æ¬¡æ€§æ¡†é€‰æ‰€æœ‰ä½œä¸šæ–‡ä»¶æ‹–å…¥ã€‚")